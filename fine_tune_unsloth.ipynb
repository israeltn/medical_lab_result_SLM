{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Fine-Tune DeepSeek R1 for Lab Test Analysis (8GB VRAM)\n",
                "\n",
                "This notebook demonstrates how to fine-tune the `unsloth/DeepSeek-R1-Distill-Llama-8B` model on a custom medical lab test dataset using Unsloth and LoRA, specifically optimized to run on an 8GB VRAM GPU."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture\n",
                "!pip install unsloth\n",
                "!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
                "!pip install datasets wandb trl peft"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Authentication\n",
                "Log in to Hugging Face and Weights & Biases (optional but recommended for tracking)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from huggingface_hub import login\n",
                "import wandb\n",
                "import os\n",
                "\n",
                "# Replace with your actual tokens or set them as environment variables\n",
                "hf_token = os.environ.get(\"HUGGINGFACE_TOKEN\", \"YOUR_HF_TOKEN_HERE\")\n",
                "login(hf_token)\n",
                "\n",
                "wb_token = os.environ.get(\"WANDB_API_KEY\", \"YOUR_WANDB_TOKEN_HERE\")\n",
                "if wb_token != \"YOUR_WANDB_TOKEN_HERE\":\n",
                "    wandb.login(key=wb_token)\n",
                "    run = wandb.init(\n",
                "        project='Fine-tune-DeepSeek-R1-Lab-Tests', \n",
                "        job_type=\"training\", \n",
                "        anonymous=\"allow\"\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Model & Tokenizer\n",
                "Using Unsloth's `FastLanguageModel` in 4-bit quantization to fit the 8B model into 8GB VRAM."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from unsloth import FastLanguageModel\n",
                "\n",
                "max_seq_length = 2048 # Adjust if you encounter OOM errors\n",
                "dtype = None # Auto-detects bf16/fp16\n",
                "load_in_4bit = True # CRITICAL for 8GB VRAM\n",
                "\n",
                "model, tokenizer = FastLanguageModel.from_pretrained(\n",
                "    model_name = \"unsloth/DeepSeek-R1-Distill-Llama-8B\",\n",
                "    max_seq_length = max_seq_length,\n",
                "    dtype = dtype,\n",
                "    load_in_4bit = load_in_4bit,\n",
                "    token = hf_token, \n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Configure LoRA Adapters\n",
                "We only train a small percentage (LoRA adapters) of the weights to save memory."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = FastLanguageModel.get_peft_model(\n",
                "    model,\n",
                "    r = 16, # LoRA rank\n",
                "    target_modules = [\n",
                "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
                "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
                "    ],\n",
                "    lora_alpha = 16,\n",
                "    lora_dropout = 0, # Dropout = 0 is recommended for Unsloth\n",
                "    bias = \"none\",    # Bias = none is recommended\n",
                "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context memory saving\n",
                "    random_state = 3407,\n",
                "    use_rslora = False,\n",
                "    loftq_config = None,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Prepare the Dataset\n",
                "Load the local JSONL conversational dataset and format it for the causal language model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from datasets import load_dataset\n",
                "\n",
                "# Load the dataset you generated\n",
                "dataset = load_dataset(\"json\", data_files=\"fine_tuning_lab_tests.jsonl\", split=\"train\")\n",
                "\n",
                "# DeepSeek-R1 specific formatting:\n",
                "prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \n",
                "Write a response that appropriately completes the request. \n",
                "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
                "\n",
                "### Instruction:\n",
                "{system}\n",
                "\n",
                "### Question:\n",
                "{user}\n",
                "\n",
                "### Response:\n",
                "<think>\n",
                "{thought}</think>\n",
                "{response}\"\"\"\n",
                "\n",
                "def formatting_prompts_func(examples):\n",
                "    texts = []\n",
                "    for messages in examples['messages']:\n",
                "        system = messages[0]['content']\n",
                "        user = messages[1]['content']\n",
                "        assistant = messages[2]['content']\n",
                "        \n",
                "        # For generating synthetic thoughts based on the final answer, \n",
                "        # we provide a generic template thought if we don't have explicit CoT data.\n",
                "        # DeepSeek values the <think> tags heavily.\n",
                "        thought_process = \"Analyzing the lab result against the reference range... Determining if the value is high, low, or normal... Formulating clinical recommendations based on standard medical guidelines for this parameter.\"\n",
                "        \n",
                "        text = prompt_style.format(system=system, user=user, thought=thought_process, response=assistant)\n",
                "        texts.append(text)\n",
                "    return { \"text\" : texts }\n",
                "\n",
                "dataset = dataset.map(formatting_prompts_func, batched = True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Training Setup\n",
                "Execute the fine-tuning process. The `per_device_train_batch_size=2` and `gradient_accumulation_steps=4` are tuned for 8GB VRAM."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from trl import SFTTrainer\n",
                "from transformers import TrainingArguments\n",
                "from unsloth import is_bfloat16_supported\n",
                "\n",
                "trainer = SFTTrainer(\n",
                "    model = model,\n",
                "    tokenizer = tokenizer,\n",
                "    train_dataset = dataset,\n",
                "    dataset_text_field = \"text\",\n",
                "    max_seq_length = max_seq_length,\n",
                "    dataset_num_proc = 2,\n",
                "    args = TrainingArguments(\n",
                "        per_device_train_batch_size = 2,\n",
                "        gradient_accumulation_steps = 4,\n",
                "        warmup_steps = 5,\n",
                "        max_steps = 60, # Increase for full training (e.g., num_train_epochs=3)\n",
                "        learning_rate = 2e-4,\n",
                "        fp16 = not is_bfloat16_supported(),\n",
                "        bf16 = is_bfloat16_supported(),\n",
                "        logging_steps = 10,\n",
                "        optim = \"adamw_8bit\", # 8-bit Adam optimizer saves VRAM\n",
                "        weight_decay = 0.01,\n",
                "        lr_scheduler_type = \"linear\",\n",
                "        seed = 3407,\n",
                "        output_dir = \"outputs\",\n",
                "    ),\n",
                ")\n",
                "\n",
                "# Start the training\n",
                "trainer_stats = trainer.train()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Inference / Testing\n",
                "Test the newly fine-tuned model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "FastLanguageModel.for_inference(model) # Enable 2x faster inference!\n",
                "\n",
                "test_system = \"You are a medical laboratory assistant. Your task is to analyze lab results, identify abnormalities based on reference ranges, and provide brief, informative explanations for healthcare professionals.\"\n",
                "test_user = \"Glucose (Fasting). Result: 155 mg/dL. Ref Range: 70-99 mg/dL.\"\n",
                "\n",
                "# Prepare prompt (without the thought and response part, so the model generates it)\n",
                "inference_prompt = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \n",
                "Write a response that appropriately completes the request. \n",
                "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
                "\n",
                "### Instruction:\n",
                "{test_system}\n",
                "\n",
                "### Question:\n",
                "{test_user}\n",
                "\n",
                "### Response:\n",
                "<think>\"\"\"\n",
                "\n",
                "inputs = tokenizer([inference_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
                "\n",
                "outputs = model.generate(\n",
                "    input_ids=inputs.input_ids,\n",
                "    attention_mask=inputs.attention_mask,\n",
                "    max_new_tokens=512,\n",
                "    use_cache=True,\n",
                ")\n",
                "\n",
                "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
                "print(response[0].split(\"### Response:\")[1])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Save and Push to Hub\n",
                "Save the LoRA adapters locally, or push them to Hugging Face."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "new_model_local = \"DeepSeek-R1-Lab-Test-Assistant-LoRA\"\n",
                "\n",
                "# Save locally\n",
                "model.save_pretrained(new_model_local) \n",
                "tokenizer.save_pretrained(new_model_local)\n",
                "\n",
                "# Optional: Save merged 16-bit model \n",
                "# model.save_pretrained_merged(new_model_local + \"-merged\", tokenizer, save_method = \"merged_16bit\")\n",
                "\n",
                "# Optional: Push to Hub\n",
                "# new_model_online = \"your_username/DeepSeek-R1-Lab-Test-Assistant-LoRA\"\n",
                "# model.push_to_hub(new_model_online)\n",
                "# tokenizer.push_to_hub(new_model_online)\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}